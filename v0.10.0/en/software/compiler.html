<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Compiler &mdash; Furiosa SDK Documentation 0.10.0 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Model Quantization" href="quantization.html" />
    <link rel="prev" title="Command Line Tools" href="cli.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Furiosa SDK Documentation
          </a>
              <div class="version">
                0.10.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">FuriosaAI NPU</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../npu/warboy.html">FuriosaAI Warboy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FuriosaAI Software</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="intro.html">FuriosaAI SW Stack Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Driver, Firmware, and Runtime Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="python-sdk.html">Python SDK installation and user guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="c-sdk.html">C SDK installation and user guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command Line Tools</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Compiler</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#furiosa-compiler"><code class="docutils literal notranslate"><span class="pre">furiosa-compiler</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-enf-files">Using ENF files</a></li>
<li class="toctree-l2"><a class="reference internal" href="#compiler-cache">Compiler Cache</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Model Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance.html">Performance Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">Performance Profiling</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/furiosa-models/latest/">FuriosaAI Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="serving.html">Model Server (Serving Framework)</a></li>
<li class="toctree-l1"><a class="reference internal" href="kubernetes_support.html">Kubernetes Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="vm_support.html">Configuring Warboy Pass-through for Virtual Machine</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorial and Code Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../releases/0.10.0.html">Release Notes - 0.10.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases/0.9.0.html">Release Notes - 0.9.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases/0.8.0.html">Release Notes - 0.8.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases/0.7.0.html">Release Notes - 0.7.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases/0.6.0.html">Release Notes - 0.6.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases/0.5.0.html">Release Notes - 0.5.0</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Customer Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.atlassian.net/servicedesk/customer/portals/">FuriosaAI Customer Center</a></li>
<li class="toctree-l1"><a class="reference internal" href="../customer-support/bugs.html">Bug Report</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Previous Documents</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/docs/v0.9.0/en/">v0.9.0</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/docs/v0.8.0/en/">v0.8.0</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/docs/v0.6.0/en/">v0.6.0</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/docs/v0.5.0/en/">v0.5.0</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/docs/v0.2.0/en/">v0.2.0</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Furiosa SDK Documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Compiler</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/software/compiler.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="compiler">
<span id="id1"></span><h1>Compiler<a class="headerlink" href="#compiler" title="Permalink to this heading"></a></h1>
<p>The FuriosaAI compiler compiles models of formats <a class="reference external" href="https://www.tensorflow.org/lite">TFLite</a> and <a class="reference external" href="https://onnx.ai/">Onnx</a> model ((<a class="reference external" href="https://github.com/onnx/onnx/blob/main/docs/Changelog.md#version-13-of-the-default-onnx-operator-set">OpSet 13</a> or lower version), thereby generating programs that execute inference using FuriosaAI NPU and resources (CPU, memory, etc) of the host machine.
In this process, the compiler analyses the model at the operator level, optimizes it, and generates a program so as to maximize NPU acceleration and host resources utilization. Even for models that are not well known,
so long as supported operators are utilized well, you can design models that are optimized for the NPU .</p>
<p>You can find the list of NPU acceleration supported operators at <a class="reference internal" href="../npu/warboy.html#supportedoperators"><span class="std std-ref">List of Supported Operators for Warboy Acceleration</span></a>.</p>
<section id="furiosa-compiler">
<span id="compilercli"></span><h2><code class="docutils literal notranslate"><span class="pre">furiosa-compiler</span></code><a class="headerlink" href="#furiosa-compiler" title="Permalink to this heading"></a></h2>
<p>The most common ways to use a compiler would be to automatically call it
during the process of resetting the inference API or the NPU.</p>
<p>But you can directly compile a model and generate a program by using the command line tool <code class="docutils literal notranslate"><span class="pre">furiosa-compiler</span></code> in shell. You can install <code class="docutils literal notranslate"><span class="pre">furiosa-compiler</span></code> command via APT package manager.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>furiosa-compiler
</pre></div>
</div>
<p>The usage of <code class="docutils literal notranslate"><span class="pre">furiosa-compiler</span></code> is as follows:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>furiosa-compiler<span class="w"> </span>--help

Furiosa<span class="w"> </span>SDK<span class="w"> </span>Compiler<span class="w"> </span>v0.10.0<span class="w"> </span><span class="o">(</span>f8f05c8ea<span class="w"> </span><span class="m">2023</span>-07-31T19:30:30Z<span class="o">)</span>

Usage:<span class="w"> </span>furiosa-compiler<span class="w"> </span><span class="o">[</span>OPTIONS<span class="o">]</span><span class="w"> </span>&lt;SOURCE&gt;

Arguments:
<span class="w">  </span>&lt;SOURCE&gt;
<span class="w">          </span>Path<span class="w"> </span>to<span class="w"> </span><span class="nb">source</span><span class="w"> </span>file<span class="w"> </span><span class="o">(</span>tflite,<span class="w"> </span>onnx,<span class="w"> </span>and<span class="w"> </span>other<span class="w"> </span>IR<span class="w"> </span>formats,<span class="w"> </span>such<span class="w"> </span>as<span class="w"> </span>dfg,<span class="w"> </span>cdfg,<span class="w"> </span>gir,<span class="w"> </span>lir<span class="o">)</span>

Options:
<span class="w">  </span>-o,<span class="w"> </span>--output<span class="w"> </span>&lt;OUTPUT&gt;
<span class="w">          </span>Writes<span class="w"> </span>output<span class="w"> </span>to<span class="w"> </span>&lt;OUTPUT&gt;

<span class="w">          </span><span class="o">[</span>default:<span class="w"> </span>output.&lt;TARGET_IR&gt;<span class="o">]</span>

<span class="w">  </span>-b,<span class="w"> </span>--batch-size<span class="w"> </span>&lt;BATCH_SIZE&gt;
<span class="w">          </span>Specifies<span class="w"> </span>the<span class="w"> </span>batch<span class="w"> </span>size<span class="w"> </span>which<span class="w"> </span>is<span class="w"> </span>effective<span class="w"> </span>when<span class="w"> </span>SOURCE<span class="w"> </span>is<span class="w"> </span>TFLite,<span class="w"> </span>ONNX,<span class="w"> </span>or<span class="w"> </span>DFG

<span class="w">      </span>--target-ir<span class="w"> </span>&lt;TARGET_IR&gt;
<span class="w">          </span><span class="o">(</span>experimental<span class="o">)</span><span class="w"> </span>Target<span class="w"> </span>IR<span class="w"> </span>-<span class="w"> </span>possible<span class="w"> </span>values:<span class="w"> </span><span class="o">[</span>enf<span class="o">]</span>

<span class="w">          </span><span class="o">[</span>default:<span class="w"> </span>enf<span class="o">]</span>

<span class="w">      </span>--target-npu<span class="w"> </span>&lt;TARGET_NPU&gt;
<span class="w">          </span>Target<span class="w"> </span>NPU<span class="w"> </span>family<span class="w"> </span>-<span class="w"> </span>possible<span class="w"> </span>values:<span class="w"> </span><span class="o">[</span>warboy,<span class="w"> </span>warboy-2pe<span class="o">]</span>

<span class="w">          </span><span class="o">[</span>default:<span class="w"> </span>warboy-2pe<span class="o">]</span>

<span class="w">      </span>--dot-graph<span class="w"> </span>&lt;DOT_GRAPH&gt;
<span class="w">          </span>Filename<span class="w"> </span>to<span class="w"> </span>write<span class="w"> </span>DOT-formatted<span class="w"> </span>graph<span class="w"> </span>to

<span class="w">      </span>--analyze-memory<span class="w"> </span>&lt;ANALYZE_MEMORY&gt;
<span class="w">          </span>Analyzes<span class="w"> </span>the<span class="w"> </span>memory<span class="w"> </span>allocation<span class="w"> </span>and<span class="w"> </span>save<span class="w"> </span>the<span class="w"> </span>report<span class="w"> </span>to<span class="w"> </span>&lt;ANALYZE_MEMORY&gt;

<span class="w">  </span>-v,<span class="w"> </span>--verbose
<span class="w">          </span>Shows<span class="w"> </span>details<span class="w"> </span>about<span class="w"> </span>the<span class="w"> </span>compilation<span class="w"> </span>process

<span class="w">      </span>--no-cache
<span class="w">          </span>Disables<span class="w"> </span>the<span class="w"> </span>compiler<span class="w"> </span>result<span class="w"> </span>cache

<span class="w">  </span>-h,<span class="w"> </span>--help
<span class="w">          </span>Print<span class="w"> </span><span class="nb">help</span><span class="w"> </span><span class="o">(</span>see<span class="w"> </span>a<span class="w"> </span>summary<span class="w"> </span>with<span class="w"> </span><span class="s1">&#39;-h&#39;</span><span class="o">)</span>

<span class="w">  </span>-V,<span class="w"> </span>--version
<span class="w">          </span>Print<span class="w"> </span>version
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">SOURCE</span></code> is the file path of
<a class="reference external" href="https://www.tensorflow.org/lite">TFLite</a> or <a class="reference external" href="https://onnx.ai/">ONNX</a>.
You have to use quantized models through <a class="reference internal" href="quantization.html#modelquantization"><span class="std std-ref">Model Quantization</span></a> for NPU accleration.</p>
<p>You can omit the option <cite>-o OUTPUT</cite>, and you can also choose to designate the output file name.
When omitted, the default output file name is <code class="docutils literal notranslate"><span class="pre">output.enf</span></code>. Here, enf stands for Executable NPU Format.
So if you run as shown below, it will generate a <code class="docutils literal notranslate"><span class="pre">output.enf</span></code> file.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>furiosa-compiler<span class="w"> </span>foo.onnx
</pre></div>
</div>
<p>If you designate the output file name as below, it will generate a <code class="docutils literal notranslate"><span class="pre">foo.enf</span></code> file.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">furiosa</span><span class="o">-</span><span class="n">compiler</span> <span class="n">foo</span><span class="o">.</span><span class="n">onnx</span> <span class="o">-</span><span class="n">o</span> <span class="n">foo</span><span class="o">.</span><span class="n">enf</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">--target-npu</span></code> lets the generated binary to designate target NPU는.</p>
<table class="docutils align-default" id="id4">
<caption><span class="caption-text">Target NPUs</span><a class="headerlink" href="#id4" title="Permalink to this table"></a></caption>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>NPU Family</p></th>
<th class="head"><p>Number of PEs</p></th>
<th class="head"><p>Value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Warboy</p></td>
<td><p>1</p></td>
<td><p>warboy</p></td>
</tr>
<tr class="row-odd"><td><p>Warboy</p></td>
<td><p>2</p></td>
<td><p>warboy-2pe</p></td>
</tr>
</tbody>
</table>
<p>If generated program’s target NPU is Warboy that uses one PE independently, you can run the following command.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">furiosa</span><span class="o">-</span><span class="n">compiler</span> <span class="n">foo</span><span class="o">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">target</span><span class="o">-</span><span class="n">npu</span> <span class="n">warboy</span>
</pre></div>
</div>
<p>When 2 PEs are fused, execute as follows.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">furiosa</span><span class="o">-</span><span class="n">compiler</span> <span class="n">foo</span><span class="o">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">target</span><span class="o">-</span><span class="n">npu</span> <span class="n">warboy</span><span class="o">-</span><span class="mi">2</span><span class="n">pe</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">--batch-size</span></code> option lets you specify <cite>batch size</cite>, the number of samples
to be passed as input when executing inference through the inference API.
The larger the batch size, the higher the NPU utilization, since more data is given as input and executed
at once. This allows the inference process to be shared across the batch, increasing efficiency.
However, if the larger batch size results in the necessary memory size exceeding NPU DRAM size,
the memory I/O cost between the host and the NPU may increase and lead to significant performance degradation.
The default value of batch size is one. Appropriate value can usually be found through trial and error.
For reference, the optimal batch sizes for some models included in the
<a class="reference external" href="https://mlcommons.org/en/inference-edge-20/">MLPerf™ Inference Edge v2.0</a> benchmark are as follows.</p>
<table class="docutils align-default" id="id5">
<caption><span class="caption-text">Optimal Batch Size for Well-known Models</span><a class="headerlink" href="#id5" title="Permalink to this table"></a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Optimal Batch</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>SSD-MobileNets-v1</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-odd"><td><p>Resnet50-v1.5</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>SSD-ResNet34</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<p>If your desired batch size is two, you can run the following command.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">furiosa</span><span class="o">-</span><span class="n">compiler</span> <span class="n">foo</span><span class="o">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">batch</span><span class="o">-</span><span class="n">size</span> <span class="mi">2</span>
</pre></div>
</div>
</section>
<section id="using-enf-files">
<h2>Using ENF files<a class="headerlink" href="#using-enf-files" title="Permalink to this heading"></a></h2>
<p>After the compilation process, the final output of the FuriosaAI compiler is ENF (Executable NPU Format) type data.
In general, the compilation process takes from a few seconds to several minutes depending on the model.
Once you have the ENF file, you can reuse it to omit this compilation process.</p>
<p>This may be useful if you need to frequently create sessions or
serve one model across several machines in an actual operation environment.</p>
<p>For example, you can first create an ENF file by referring to <a class="reference internal" href="#compilercli"><span class="std std-ref">furiosa-compiler</span></a>.
Then, with <a class="reference internal" href="python-sdk.html#pythonsdk"><span class="std std-ref">PythonSDK</span></a> as shown below,
you can instantly create a runner without the compilation process by
passing the ENF file as an argument to the <code class="docutils literal notranslate"><span class="pre">create_runner()</span></code> function as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">furiosa.runtime</span> <span class="kn">import</span> <span class="n">sync</span>

<span class="k">with</span> <span class="n">sync</span><span class="o">.</span><span class="n">create_runner</span><span class="p">(</span><span class="s2">&quot;path/to/model.enf&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">runner</span><span class="p">:</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="n">runner</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="compiler-cache">
<span id="compilercache"></span><h2>Compiler Cache<a class="headerlink" href="#compiler-cache" title="Permalink to this heading"></a></h2>
<p>Compiler cache allows to user applications to reuse once-compiled results.
It’s very helpful especially when you are developing applications because the compilation
usually takes at least a couple of minutes.</p>
<p>By default, the compiler cache uses a local file system (<code class="docutils literal notranslate"><span class="pre">$HOME/.cache/furiosa/compiler</span></code>) as a cache storage.
If you specify a configuration, you can also use Redis as a remote and distributed cache storage.</p>
<p>The compiler cache is enabled by default, but you can explicitly enable or disable the cache by setting <code class="docutils literal notranslate"><span class="pre">FC_CACHE_ENABLED</span></code>.
This setting is effective in CLI tools, Python SDK, and serving frameworks.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="c1"># Enable Compiler Cache</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">FC_CACHE_ENABLED</span><span class="o">=</span><span class="m">1</span>
<span class="c1"># Disable Compiler Cache</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">FC_CACHE_ENABLED</span><span class="o">=</span><span class="m">0</span>
</pre></div>
</div>
<p>The default cache location is <code class="docutils literal notranslate"><span class="pre">$HOME/.cache/furiosa/compiler</span></code>, but you can explicitly specify the cache storage
by setting the shell environment variable <code class="docutils literal notranslate"><span class="pre">FC_CACHE_STORE_URL</span></code>. If you want to Redis as a cache storage,
you can specify some URLs starting with <code class="docutils literal notranslate"><span class="pre">redis://</span></code> or <code class="docutils literal notranslate"><span class="pre">rediss://</span></code> (over SSL).</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="c1"># When you want to specify a cache directory</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">FC_CACHE_STORE_URL</span><span class="o">=</span>/tmp/cache

<span class="c1"># When you want to specify a Redis cluster as the cache storage</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">FC_CACHE_STORE_URL</span><span class="o">=</span>redis://:&lt;PASSWORD&gt;@127.0.0.1:6379
<span class="c1"># When you want to specify a Redis cluster over SSL as the cache storage</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">FC_CACHE_STORE_URL</span><span class="o">=</span>rediss://:&lt;PASSWORD&gt;@127.0.0.1:25945
</pre></div>
</div>
<p>The cache will be valid for 30 days by default, but you can explicitly specify the cache lifetime by setting
seconds to the environment variable <code class="docutils literal notranslate"><span class="pre">FC_CACHE_LIFETIME</span></code>.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="c1"># 2 hours cache lifetime</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">FC_CACHE_LIFETIME</span><span class="o">=</span><span class="m">7200</span>
</pre></div>
</div>
<p>Also, you can control more the cache behavior according to your purpose as follows:</p>
<table class="docutils align-default" id="id6">
<caption><span class="caption-text">Cache behaviors according to <code class="docutils literal notranslate"><span class="pre">FC_CACHE_LIFETIME</span></code></span><a class="headerlink" href="#id6" title="Permalink to this table"></a></caption>
<colgroup>
<col style="width: 17%" />
<col style="width: 67%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Value (secs)</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Example</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><em>N</em> &gt; 0</p></td>
<td><p>Cache will be alive for N secs</p></td>
<td><p>7200 (2 hours)</p></td>
</tr>
<tr class="row-odd"><td><p>0</p></td>
<td><p>All previous cache will be invalidated. (When you want to compile the model without cache)</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p><em>N</em> &lt; 0</p></td>
<td><p>Cache will be alive forever without expiration. (it can be useful when you want read-only cache)</p></td>
<td><p>-1</p></td>
</tr>
</tbody>
</table>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="cli.html" class="btn btn-neutral float-left" title="Command Line Tools" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="quantization.html" class="btn btn-neutral float-right" title="Model Quantization" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023 FuriosaAI, Inc..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>