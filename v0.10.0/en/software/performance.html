<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Performance Optimization &mdash; Furiosa SDK Documentation 0.11.0.dev0 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Performance Profiling" href="profiler.html" />
    <link rel="prev" title="Model Quantization" href="quantization.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Furiosa SDK Documentation
          </a>
              <div class="version">
                0.11.0.dev0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">FuriosaAI NPU</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../npu/warboy.html">FuriosaAI Warboy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FuriosaAI Software</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="intro.html">FuriosaAI SW Stack Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Driver, Firmware, and Runtime Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="python-sdk.html">Python SDK installation and user guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="c-sdk.html">C SDK installation and user guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command Line Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiler.html">Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Model Quantization</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Performance Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#performance-metrics-latency-and-throughput">Performance Metrics: Latency and Throughput</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#performance-profiling">Performance Profiling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-optimization">Model Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#optimizing-quantize-operator">Optimizing <code class="docutils literal notranslate"><span class="pre">Quantize</span></code> Operator</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optimizing-dequantize-operator">Optimizing <code class="docutils literal notranslate"><span class="pre">Dequantize</span></code> Operator</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lower-unlower-acceleration">Lower/Unlower Acceleration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#removal-of-pad-slice">Removal of Pad/Slice</a></li>
<li class="toctree-l3"><a class="reference internal" href="#change-the-order-of-input-tensor-axes-at-compiler-time">Change the Order of Input Tensor Axes at Compiler Time</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optimization-of-large-input-and-output-tensors">Optimization of Large Input and Output Tensors</a></li>
<li class="toctree-l3"><a class="reference internal" href="#more-batch-more-npu-utilization">More Batch, More NPU Utilization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#single-pe-vs-fusion-pe">Single PE vs Fusion PE</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#runtime-optimization">Runtime Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#more-inference-concurrency-the-number-of-workers">More inference concurrency (the number of workers)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sync-api-vs-async-apis">Sync API vs Async APIs</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">Performance Profiling</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/furiosa-models/latest/">FuriosaAI Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="serving.html">Model Server (Serving Framework)</a></li>
<li class="toctree-l1"><a class="reference internal" href="kubernetes_support.html">Kubernetes Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="vm_support.html">Configuring Warboy Pass-through for Virtual Machine</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorial and Code Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../releases/0.10.0.html">Release Notes - 0.10.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases/0.9.0.html">Release Notes - 0.9.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases/0.8.0.html">Release Notes - 0.8.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases/0.7.0.html">Release Notes - 0.7.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases/0.6.0.html">Release Notes - 0.6.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases/0.5.0.html">Release Notes - 0.5.0</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Customer Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.atlassian.net/servicedesk/customer/portals/">FuriosaAI Customer Center</a></li>
<li class="toctree-l1"><a class="reference internal" href="../customer-support/bugs.html">Bug Report</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Previous Documents</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/docs/v0.9.0/en/">v0.9.0</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/docs/v0.8.0/en/">v0.8.0</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/docs/v0.6.0/en/">v0.6.0</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/docs/v0.5.0/en/">v0.5.0</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/docs/v0.2.0/en/">v0.2.0</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Furiosa SDK Documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Performance Optimization</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/software/performance.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="performance-optimization">
<span id="performanceoptimization"></span><h1>Performance Optimization<a class="headerlink" href="#performance-optimization" title="Permalink to this heading"></a></h1>
<p>To ensure efficient inference serving in production,
it’s essential to focus on throughput and latency as key metrics.
Furiosa SDK offers two optimization methods for both throughput and latency:</p>
<ul class="simple">
<li><p><strong>Model Optimization</strong>: are ways to optimize models during the phases of model development,
quantization, and compilation. Some optimization techniques may modify the models, leading to
more efficient compiled programs.</p></li>
<li><p><strong>Runtime Optimization</strong>: are ways to optimize the runtime execution of compiled programs.
They are about how to optimize inference codes through Runtime library depending
on the characteristics of models workloads for higher throughput.</p></li>
</ul>
<p>In this section, we will discuss the performance metrics and how to optimize them
in both above ways.</p>
<section id="performance-metrics-latency-and-throughput">
<h2>Performance Metrics: Latency and Throughput<a class="headerlink" href="#performance-metrics-latency-and-throughput" title="Permalink to this heading"></a></h2>
<p><em>Latency</em> is one of the major performance evaluation criteria for model inference.
it’s a measure of how long a single inference takes
from when the input data is passed to the model until the output value is received.
With low latency, users can experience high responsiveness.</p>
<p>Another performance evaluation criterion is throughput.
Throughput means the number of inferences that can be processed within a unit of time.
Throughput implies that how many requests a system handle simultaneously.</p>
<p>A single end-to-end inference consists of three kinds of operations: NPU execution, CPU computation
and IO operation between host and NPU device. Three kinds of operations run independently without blocking one another.
So, multiple inferences can run simultaneously while different operations run.
When we continue to run multiple requests simultaneously,
the longer operation among NPU, CPU, and IO operations is likely to determine the inference time.
This is because the shorter operations will be hidden by other longer operations.
This is a key characteristic to understand how to optimize the performance of a model.
You can find more details at <a class="reference internal" href="#concurrencyoptimization"><span class="std std-ref">Concurrency Optimization</span></a>.</p>
<p>NPU utilization is not a performance metrics, but it’s one of the key metrics to indicate
how much the model utilizes a NPU device for inferences.
NPU utilization can be defined as the proportion of time the NPU is used during inference.
With NPU utilziation, we can evaluate how well the model is well-optimized for NPU acceleration.
Sometimes, it may also imply the room for further optimization opportunities.
Please refer to <a class="reference internal" href="cli.html#toolkit"><span class="std std-ref">Toolkit</span></a> for how to measure NPU utilization.</p>
<section id="performance-profiling">
<h3>Performance Profiling<a class="headerlink" href="#performance-profiling" title="Permalink to this heading"></a></h3>
<p>To analyze the performance of a workload, we need to measure performance metrics as well as
we need to have a closer look at the times of NPU executions, CPU computations, and I/O operations.</p>
<p>For them, there are two useful tools in Furiosa SDK.</p>
<ul class="simple">
<li><p><a class="reference internal" href="cli.html#furiosabench"><span class="std std-ref">furiosa-bench (Benchmark Tool)</span></a> is a tool to measure the performance metrics such as latencies and
throughput (i.e., QPS - queries per second).</p></li>
<li><p><a class="reference internal" href="profiler.html#profiling"><span class="std std-ref">Performance Profiling</span></a> provides a way to measure the durations of NPU executions and other operations.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">furiosa-bench</span></code> also provides <code class="docutils literal notranslate"><span class="pre">--trace-output</span></code> option to generate a trace file.
This is another easy way to measure the durations of NPU executions and other operations
from a running workload.</p>
</section>
</section>
<section id="model-optimization">
<h2>Model Optimization<a class="headerlink" href="#model-optimization" title="Permalink to this heading"></a></h2>
<p>In this seciton, we introduce some model optimization techniques to improve the performance
of models. They key idea of the model optimization is to identify the bottleneck parts
(usually operators) of the model and to reduce the times of the bottleneck parts or remove them.</p>
<p>For example, if some operators of a model are not accelerated by NPU,
they can be a major bottleneck. If you remove the operators or replace them with other equivalents,
the inference latency can be reduced significantly.</p>
<section id="optimizing-quantize-operator">
<h3>Optimizing <code class="docutils literal notranslate"><span class="pre">Quantize</span></code> Operator<a class="headerlink" href="#optimizing-quantize-operator" title="Permalink to this heading"></a></h3>
<p>FuriosaAI’s first-generation NPU, Warboy, supports only int8 type.
As the majority of deep learning models are built upon floating point types like fp32 and fp16,
to execute these models on Warboy,
a quantization step is necessary to convert the fp32 weights to int8 model weights.
In addition, the quantization step adds <code class="docutils literal notranslate"><span class="pre">quantize</span></code>, <code class="docutils literal notranslate"><span class="pre">dequantize</span></code> operators to the
input and output parts of the model respectively.
<code class="docutils literal notranslate"><span class="pre">quantize</span></code> and <code class="docutils literal notranslate"><span class="pre">dequantize</span></code> operators convert fp32 input values to int8 values and vice versa.
Those operators are executed on the CPU and are time-consuming.</p>
<p>Inputs of many CNN-based models are images. In particular, an image is represented as
RGB channels. In other words, a single image is composed of three images for each channel of RGB,
where each image is represented with 8-bit integer values, ranging from 0 to 255.</p>
<p>To feed an image to a model, we need to convert the <code class="docutils literal notranslate"><span class="pre">int8</span></code> values of each RGB channel
to <code class="docutils literal notranslate"><span class="pre">fp32</span></code> values, and <code class="docutils literal notranslate"><span class="pre">quantize</span></code> operator in the model converts <code class="docutils literal notranslate"><span class="pre">fp32</span></code> values
to <code class="docutils literal notranslate"><span class="pre">int8</span></code> values. It’s unnecessary if we can feed RGB images in <code class="docutils literal notranslate"><span class="pre">int8</span></code> to a model directly.</p>
<p>To support this optimization, <code class="docutils literal notranslate"><span class="pre">furiosa-quantizer</span></code> provides the <code class="docutils literal notranslate"><span class="pre">ModelEditor</span></code> API.
<code class="docutils literal notranslate"><span class="pre">ModelEditor</span></code> takes the model optimized by <code class="docutils literal notranslate"><span class="pre">optimize_model()</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;yolox_l.onnx&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">optimize_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">editor</span> <span class="o">=</span> <span class="n">ModelEditor</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">convert_input_type()</span></code> method of <code class="docutils literal notranslate"><span class="pre">ModelEditor</span></code> takes a tensor name and a data type as arguments.
It modifies the data type of the input tensor in the model to be the given arguments.
The target type can be either <code class="docutils literal notranslate"><span class="pre">INT8</span></code> or <code class="docutils literal notranslate"><span class="pre">UINT8</span></code>. You can find the tensor name
through <code class="docutils literal notranslate"><span class="pre">get_pure_input_names</span></code> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">input_tensor_name</span> <span class="o">=</span> <span class="n">get_pure_input_names</span><span class="p">(</span><span class="n">model</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Convert this input tensor to uint8</span>
<span class="n">editor</span><span class="o">.</span><span class="n">convert_input_type</span><span class="p">(</span><span class="n">input_tensor_name</span><span class="p">,</span> <span class="n">TensorType</span><span class="o">.</span><span class="n">UINT8</span><span class="p">)</span>
</pre></div>
</div>
<p>As you can see in the above example, <code class="docutils literal notranslate"><span class="pre">convert_input_type</span></code> changes the data type of the input tensor to be <code class="docutils literal notranslate"><span class="pre">uint8</span></code>.
The reason why we use <code class="docutils literal notranslate"><span class="pre">uint8</span></code> instead of <code class="docutils literal notranslate"><span class="pre">int8</span></code> is that the pixel values are represented as positive values.</p>
<p>Before this model modification, <code class="docutils literal notranslate"><span class="pre">quantize</span></code> operator converts <code class="docutils literal notranslate"><span class="pre">float32</span></code> values to <code class="docutils literal notranslate"><span class="pre">int8</span></code> values.
After this model modification, the quantize operator converts <code class="docutils literal notranslate"><span class="pre">uint8</span></code> values to <code class="docutils literal notranslate"><span class="pre">int8</span></code> values.
This conversion from <code class="docutils literal notranslate"><span class="pre">uint8</span></code> to <code class="docutils literal notranslate"><span class="pre">int8</span></code> is much faster than the conversion from <code class="docutils literal notranslate"><span class="pre">float32</span></code> to <code class="docutils literal notranslate"><span class="pre">int8</span></code>.
The followings are the the benchmark results of before and after the model modification.
Also, the figure shows how the <code class="docutils literal notranslate"><span class="pre">quantize</span></code> operator is changed.</p>
<table class="docutils align-center" id="id2">
<caption><span class="caption-text">Quantization in YOLOX_L</span><a class="headerlink" href="#id2" title="Permalink to this table"></a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Input type</p></th>
<th class="head"><p><code class="docutils literal notranslate"><span class="pre">Quantize</span></code> execution time</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>float32</p></td>
<td><p>60.639 ms</p></td>
</tr>
<tr class="row-odd"><td><p>uint8</p></td>
<td><p>0.277 ms</p></td>
</tr>
</tbody>
</table>
<figure class="align-center" id="id3">
<img alt="../_images/quantize_0.png" class="with-border" src="../_images/quantize_0.png" />
<figcaption>
<p><span class="caption-text">quantize without <code class="docutils literal notranslate"><span class="pre">ModelEditor</span></code></span><a class="headerlink" href="#id3" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id4">
<img alt="../_images/quantize_1.png" class="with-border" src="../_images/quantize_1.png" />
<figcaption>
<p><span class="caption-text">quantize with <code class="docutils literal notranslate"><span class="pre">convert_input_type</span></code></span><a class="headerlink" href="#id4" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This optmization may affect the accurarcy of the model.
Since it depends on models and applications,
it is recommended to validate the accuracy of the model.</p>
</div>
<p>The following is a real example code to use <code class="docutils literal notranslate"><span class="pre">ModelEditor</span></code> API with <code class="docutils literal notranslate"><span class="pre">convert_input_type()</span></code>.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>

<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">onnx</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">tqdm</span>

<span class="kn">from</span> <span class="nn">furiosa.optimizer</span> <span class="kn">import</span> <span class="n">optimize_model</span>
<span class="kn">from</span> <span class="nn">furiosa.quantizer</span> <span class="kn">import</span> <span class="n">get_pure_input_names</span><span class="p">,</span> <span class="n">quantize</span><span class="p">,</span> <span class="n">Calibrator</span><span class="p">,</span> <span class="n">CalibrationMethod</span><span class="p">,</span> <span class="n">ModelEditor</span><span class="p">,</span> <span class="n">TensorType</span>
<span class="kn">from</span> <span class="nn">furiosa.runtime</span> <span class="kn">import</span> <span class="n">session</span>
<span class="kn">from</span> <span class="nn">furiosa.runtime.profiler</span> <span class="kn">import</span> <span class="n">profile</span>


<span class="n">torch_model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s1">&#39;DEFAULT&#39;</span><span class="p">)</span>
<span class="n">torch_model</span> <span class="o">=</span> <span class="n">torch_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">dummy_input</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
    <span class="n">torch_model</span><span class="p">,</span>  <span class="c1"># PyTorch model to export</span>
    <span class="n">dummy_input</span><span class="p">,</span>  <span class="c1"># model input</span>
    <span class="s2">&quot;resnet50.onnx&quot;</span><span class="p">,</span>  <span class="c1"># where to save the exported ONNX model</span>
    <span class="n">opset_version</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span>  <span class="c1"># the ONNX OpSet version to export the model to</span>
    <span class="n">do_constant_folding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># whether to execute constant folding for optimization</span>
    <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">],</span>  <span class="c1"># the ONNX model&#39;s input names</span>
    <span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">],</span>  <span class="c1"># the ONNX model&#39;s output names</span>
<span class="p">)</span>

<span class="n">onnx_model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;resnet50.onnx&quot;</span><span class="p">)</span>
<span class="n">onnx_model</span> <span class="o">=</span> <span class="n">optimize_model</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">)</span>

<span class="n">calibrator</span> <span class="o">=</span> <span class="n">Calibrator</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">,</span> <span class="n">CalibrationMethod</span><span class="o">.</span><span class="n">MIN_MAX_ASYM</span><span class="p">)</span>
<span class="n">calibrator</span><span class="o">.</span><span class="n">collect_data</span><span class="p">([[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]])</span>
<span class="n">ranges</span> <span class="o">=</span> <span class="n">calibrator</span><span class="o">.</span><span class="n">compute_range</span><span class="p">()</span>

<span class="n">editor</span> <span class="o">=</span> <span class="n">ModelEditor</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">)</span>
<span class="n">input_tensor_name</span> <span class="o">=</span> <span class="n">get_pure_input_names</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Convert the input type to uint8</span>
<span class="n">editor</span><span class="o">.</span><span class="n">convert_input_type</span><span class="p">(</span><span class="n">input_tensor_name</span><span class="p">,</span> <span class="n">TensorType</span><span class="o">.</span><span class="n">UINT8</span><span class="p">)</span>

<span class="n">graph</span> <span class="o">=</span> <span class="n">quantize</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">,</span> <span class="n">ranges</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;trace.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">trace</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">profile</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="n">trace</span><span class="p">)</span> <span class="k">as</span> <span class="n">profiler</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">session</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
            <span class="n">image</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">profiler</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="s2">&quot;pre&quot;</span><span class="p">):</span>
                <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">profiler</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">):</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">profiler</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="s2">&quot;post&quot;</span><span class="p">):</span>
                <span class="n">prediction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="optimizing-dequantize-operator">
<h3>Optimizing <code class="docutils literal notranslate"><span class="pre">Dequantize</span></code> Operator<a class="headerlink" href="#optimizing-dequantize-operator" title="Permalink to this heading"></a></h3>
<p>Similar to the above <code class="docutils literal notranslate"><span class="pre">Quantize</span></code> operator optimization,
<code class="docutils literal notranslate"><span class="pre">Dequantize</span></code> operator also can be optimized in the similar way.</p>
<p>If the model output tensor is <code class="docutils literal notranslate"><span class="pre">fp32</span></code>, the output of <code class="docutils literal notranslate"><span class="pre">int8</span></code> values must be converted to f32 values.
<code class="docutils literal notranslate"><span class="pre">Dequantize</span></code> operator converts int8 values to fp32 values, and it’s executed on CPU.
If the model output is an RGB image or something else which can be represented as <code class="docutils literal notranslate"><span class="pre">int8</span></code> or <code class="docutils literal notranslate"><span class="pre">uint8</span></code> values,
we can skip converting <code class="docutils literal notranslate"><span class="pre">int8</span></code> or <code class="docutils literal notranslate"><span class="pre">uint8</span></code> to <code class="docutils literal notranslate"><span class="pre">fp32</span></code>. It will reduce the inference latency significantly.</p>
<p>We can enable this optimization by using <code class="docutils literal notranslate"><span class="pre">convert_output_type()</span></code> method of <code class="docutils literal notranslate"><span class="pre">ModelEditor</span></code>.
<code class="docutils literal notranslate"><span class="pre">convert_output_type()</span></code> method can modifies a model output by a given tensor name and a target data type.
The target type can be either <code class="docutils literal notranslate"><span class="pre">INT8</span></code> or <code class="docutils literal notranslate"><span class="pre">UINT8</span></code>.</p>
<figure class="align-center" id="id5">
<img alt="../_images/quantize_2.png" class="with-border" src="../_images/quantize_2.png" />
<figcaption>
<p><span class="caption-text">quantize with <code class="docutils literal notranslate"><span class="pre">convert_output_type</span></code></span><a class="headerlink" href="#id5" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id6">
<img alt="../_images/quantize_3.png" class="with-border" src="../_images/quantize_3.png" />
<figcaption>
<p><span class="caption-text">quantize with <code class="docutils literal notranslate"><span class="pre">convert_input_type</span></code> and <code class="docutils literal notranslate"><span class="pre">convert_output_type</span></code></span><a class="headerlink" href="#id6" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Furiosa Compiler may automatically apply this optimization
to the model even if this optmization is not explicitly applied.
In that case, the optimization by Furiosa Compiler may result in lower latency
than the one manually applied by <code class="docutils literal notranslate"><span class="pre">ModelEditor</span></code>.
It is recommended to do experiments to find the best option.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This optmization may affect the accurarcy of the model.
Since it depends on models and applications,
it is recommended to validate the accuracy of the model.</p>
</div>
<p>The following is an real example code to use <code class="docutils literal notranslate"><span class="pre">convert_output_type</span></code> option.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>

<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">onnx</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">tqdm</span>

<span class="kn">from</span> <span class="nn">furiosa.optimizer</span> <span class="kn">import</span> <span class="n">optimize_model</span>
<span class="kn">from</span> <span class="nn">furiosa.quantizer</span> <span class="kn">import</span> <span class="n">get_output_names</span><span class="p">,</span> <span class="n">quantize</span><span class="p">,</span> <span class="n">Calibrator</span><span class="p">,</span> <span class="n">CalibrationMethod</span><span class="p">,</span> <span class="n">ModelEditor</span><span class="p">,</span> <span class="n">TensorType</span>
<span class="kn">from</span> <span class="nn">furiosa.runtime</span> <span class="kn">import</span> <span class="n">session</span>
<span class="kn">from</span> <span class="nn">furiosa.runtime.profiler</span> <span class="kn">import</span> <span class="n">profile</span>


<span class="n">torch_model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s1">&#39;DEFAULT&#39;</span><span class="p">)</span>
<span class="n">torch_model</span> <span class="o">=</span> <span class="n">torch_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">dummy_input</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
    <span class="n">torch_model</span><span class="p">,</span>  <span class="c1"># PyTorch model to export</span>
    <span class="n">dummy_input</span><span class="p">,</span>  <span class="c1"># model input</span>
    <span class="s2">&quot;resnet50.onnx&quot;</span><span class="p">,</span>  <span class="c1"># where to save the exported ONNX model</span>
    <span class="n">opset_version</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span>  <span class="c1"># the ONNX OpSet version to export the model to</span>
    <span class="n">do_constant_folding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># whether to execute constant folding for optimization</span>
    <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">],</span>  <span class="c1"># the ONNX model&#39;s input names</span>
    <span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">],</span>  <span class="c1"># the ONNX model&#39;s output names</span>
<span class="p">)</span>

<span class="n">onnx_model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;resnet50.onnx&quot;</span><span class="p">)</span>
<span class="n">onnx_model</span> <span class="o">=</span> <span class="n">optimize_model</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">)</span>

<span class="n">calibrator</span> <span class="o">=</span> <span class="n">Calibrator</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">,</span> <span class="n">CalibrationMethod</span><span class="o">.</span><span class="n">MIN_MAX_ASYM</span><span class="p">)</span>
<span class="n">calibrator</span><span class="o">.</span><span class="n">collect_data</span><span class="p">([[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]])</span>
<span class="n">ranges</span> <span class="o">=</span> <span class="n">calibrator</span><span class="o">.</span><span class="n">compute_range</span><span class="p">()</span>

<span class="n">editor</span> <span class="o">=</span> <span class="n">ModelEditor</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">)</span>
<span class="n">output_tensor_name</span> <span class="o">=</span> <span class="n">get_output_names</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># output 텐서의 자료형을 int8로 변환</span>
<span class="n">editor</span><span class="o">.</span><span class="n">convert_output_type</span><span class="p">(</span><span class="n">output_tensor_name</span><span class="p">,</span> <span class="n">TensorType</span><span class="o">.</span><span class="n">INT8</span><span class="p">)</span>

<span class="n">graph</span> <span class="o">=</span> <span class="n">quantize</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">,</span> <span class="n">ranges</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;trace.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">trace</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">profile</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="n">trace</span><span class="p">)</span> <span class="k">as</span> <span class="n">profiler</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">session</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
            <span class="n">image</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">profiler</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="s2">&quot;pre&quot;</span><span class="p">):</span>
                <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">profiler</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">):</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">profiler</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="s2">&quot;post&quot;</span><span class="p">):</span>
                <span class="n">prediction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="lower-unlower-acceleration">
<h3>Lower/Unlower Acceleration<a class="headerlink" href="#lower-unlower-acceleration" title="Permalink to this heading"></a></h3>
<p>Warboy internally uses its inherent memory layout to accelerate the computation
by leveraging the NPU architecture.
For the memory layout, <code class="docutils literal notranslate"><span class="pre">Lower</span></code> operator reshapes the input tensor to the NPU’s memory layout and
<code class="docutils literal notranslate"><span class="pre">Unlower</span></code> operator reshapes the output tensor from the NPU’s memory layout to the original shape.
For them, Furiosa Compiler automatically adds <code class="docutils literal notranslate"><span class="pre">Lower</span></code> and <code class="docutils literal notranslate"><span class="pre">Unlower</span></code> operators to the model.</p>
<p>In many cases, <code class="docutils literal notranslate"><span class="pre">Lower</span></code> and <code class="docutils literal notranslate"><span class="pre">Unlower</span></code> are executed on CPU, causing some overhead
of the inference latency.
However, if the last axis of input or output tensor shape is <code class="docutils literal notranslate"><span class="pre">width</span></code> and
the size of the last axis is a multiple of 32, <code class="docutils literal notranslate"><span class="pre">Lower</span></code> and <code class="docutils literal notranslate"><span class="pre">Unlower</span></code> operators can be accleerated on NPU.
Then, the inference latency can be reduced significantly.</p>
<p>Therefore, if you are able to specify the shape of the input and output tensors,
it’s more optimal to use <code class="docutils literal notranslate"><span class="pre">NxCxHxW</span></code> and specify the width as a multiple of 32.
Also, this optimization can be applied independently to the input and output tensors respectively.</p>
</section>
<section id="removal-of-pad-slice">
<h3>Removal of Pad/Slice<a class="headerlink" href="#removal-of-pad-slice" title="Permalink to this heading"></a></h3>
<p>As described above, the <code class="docutils literal notranslate"><span class="pre">Lower</span></code> / <code class="docutils literal notranslate"><span class="pre">Unlower</span></code> operations can be accelerated
if the last axis of the tensor for either operator is <cite>width</cite> and
the size of the last axis is a multiple of 32.</p>
<p>If the last tensor axis of <code class="docutils literal notranslate"><span class="pre">Lower</span></code> is <cite>width</cite> but not a multiple of 32,
Furiosa Compiler may automatically add <code class="docutils literal notranslate"><span class="pre">Pad</span></code> operator before <code class="docutils literal notranslate"><span class="pre">Lower</span></code> operator
to adjust the size of the last axis to a multiple of 32.
In the similar way, Furiosa Compiler may automatically add <code class="docutils literal notranslate"><span class="pre">Slice</span></code> operator after <code class="docutils literal notranslate"><span class="pre">Unlower</span></code> operator to
slice data contents from the tensor with the last axis of a multiple of 32 to the original tensor shape.</p>
<p>This optimization gains some performance benefits by accelerating <code class="docutils literal notranslate"><span class="pre">Lower</span></code> / <code class="docutils literal notranslate"><span class="pre">Unlower</span></code> operations.
However, <code class="docutils literal notranslate"><span class="pre">Pad</span></code> and <code class="docutils literal notranslate"><span class="pre">Slice</span></code> requires CPU computation.
There’s futher optimization opportunity to remove even <code class="docutils literal notranslate"><span class="pre">Pad</span></code> and <code class="docutils literal notranslate"><span class="pre">Slice</span></code> operators too.
If you can accept the constraints of the input and output tensor shapes,
it is strongly recommended using the shape of the tensors <code class="docutils literal notranslate"><span class="pre">NxCxHxW</span></code> and
a multiple of 32 of the width.</p>
</section>
<section id="change-the-order-of-input-tensor-axes-at-compiler-time">
<h3>Change the Order of Input Tensor Axes at Compiler Time<a class="headerlink" href="#change-the-order-of-input-tensor-axes-at-compiler-time" title="Permalink to this heading"></a></h3>
<p>As we discussed above, there are more optimization opportunities
if the last axis of the input tensor is <code class="docutils literal notranslate"><span class="pre">width</span></code>.
However, changing the order of axes requires to modify the models.
It may require some effort to modify the original models in some cases.</p>
<p>So, Furiosa Compiler provides a way to change the order of the input tensor axes
at compile time. You can specify <code class="docutils literal notranslate"><span class="pre">permute_input</span></code> option in compiler config
to specify the new order of the input tensor axes as follows:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">compiler_config</span> <span class="pre">=</span> <span class="pre">{</span> <span class="pre">&quot;permute_input&quot;:</span> <span class="pre">[[0,</span> <span class="pre">3,</span> <span class="pre">1,</span> <span class="pre">2]]</span> <span class="pre">}</span></code></p>
<blockquote>
<div><ul class="simple">
<li><p>The parameter of <code class="docutils literal notranslate"><span class="pre">permute_input</span></code> is the same as <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.permute.html">torch.permute</a>.</p></li>
<li><p>For example, the above example code will change <code class="docutils literal notranslate"><span class="pre">NxHxWxC</span></code> to <code class="docutils literal notranslate"><span class="pre">NxCxHxW</span></code>.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The following is a real example code to use <code class="docutils literal notranslate"><span class="pre">permute_input</span></code> option.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>

<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">onnx</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">tqdm</span>

<span class="kn">from</span> <span class="nn">furiosa.optimizer</span> <span class="kn">import</span> <span class="n">optimize_model</span>
<span class="kn">from</span> <span class="nn">furiosa.quantizer</span> <span class="kn">import</span> <span class="n">quantize</span><span class="p">,</span> <span class="n">Calibrator</span><span class="p">,</span> <span class="n">CalibrationMethod</span>
<span class="kn">from</span> <span class="nn">furiosa.runtime</span> <span class="kn">import</span> <span class="n">session</span>
<span class="kn">from</span> <span class="nn">furiosa.runtime.profiler</span> <span class="kn">import</span> <span class="n">profile</span>


<span class="n">onnx_model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;model_nhwc.onnx&quot;</span><span class="p">)</span>
<span class="n">onnx_model</span> <span class="o">=</span> <span class="n">optimize_model</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">)</span>

<span class="n">calibrator</span> <span class="o">=</span> <span class="n">Calibrator</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">,</span> <span class="n">CalibrationMethod</span><span class="o">.</span><span class="n">MIN_MAX_ASYM</span><span class="p">)</span>
<span class="n">calibrator</span><span class="o">.</span><span class="n">collect_data</span><span class="p">([[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]])</span>
<span class="n">ranges</span> <span class="o">=</span> <span class="n">calibrator</span><span class="o">.</span><span class="n">compute_range</span><span class="p">()</span>

<span class="n">graph</span> <span class="o">=</span> <span class="n">quantize</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">,</span> <span class="n">ranges</span><span class="p">)</span>

<span class="n">compiler_config</span> <span class="o">=</span> <span class="p">{</span> <span class="s2">&quot;permute_input&quot;</span><span class="p">:</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span> <span class="p">}</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;trace.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">trace</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">profile</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="n">trace</span><span class="p">)</span> <span class="k">as</span> <span class="n">profiler</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">session</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">compiler_config</span><span class="o">=</span><span class="n">compiler_config</span><span class="p">)</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
            <span class="n">image</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">profiler</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="s2">&quot;pre&quot;</span><span class="p">):</span>
                <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">profiler</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">):</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">profiler</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="s2">&quot;post&quot;</span><span class="p">):</span>
                <span class="n">prediction</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
<p>This is another case to use <code class="docutils literal notranslate"><span class="pre">permute_input</span></code> option.
In some cases, it’s necessary to change the order of the input tensor axes
from <code class="docutils literal notranslate"><span class="pre">NxCxHxW</span></code> to <code class="docutils literal notranslate"><span class="pre">NxHxWxC</span></code>.
Python OpenCV is a popular computer vision library.
<code class="docutils literal notranslate"><span class="pre">cv2.imread()</span></code> of OpenCV returns a 3D NumPy array with <code class="docutils literal notranslate"><span class="pre">HxWxC</span></code> order.
If the axes of the input tensors of a model are <code class="docutils literal notranslate"><span class="pre">NxCxHxW</span></code>, it requires to transpose the tensor.
The transpose is a time-consuming operation running in CPU.
In this case, we can remove the transpose operation
if we change the order of the input tensor axes of the model to the same as
OpenCV’s output; e.g., <code class="docutils literal notranslate"><span class="pre">NxHxWxC</span></code>. It will reduce the inference latency significantly.</p>
</section>
<section id="optimization-of-large-input-and-output-tensors">
<h3>Optimization of Large Input and Output Tensors<a class="headerlink" href="#optimization-of-large-input-and-output-tensors" title="Permalink to this heading"></a></h3>
<p>Some models have large images and as inputs and outputs. For example,
Denoising and super resolution models basically take large images as inputs and outputs.
Depending on your implementation, those models may be slow in Furiosa SDK and Warboy.
Furiosa Compiler optimizes the models with various techniques
while preserving the semantics of the original models.
Basically, Furiosa Compiler handles large tensors as defined by the model.
However, if the size of tensors is too large, it may exceed SRAM memory of Warboy,
causing more I/O operations between DRAM and SRAM. It may result in poor performance.</p>
<p>We can optimize this case by splitting a large tensor
into a number of smaller tensors and then merging the results.
Generally, we can apply this optimization to denosing and super resolution models
because the small parts of images can be independently processed and merged to get the final results.
The small parts of images are called patches, and the size of patches is called patch size.</p>
<p>To understand the optimization mechanism, we need to understand how the Furiosa Compiler works.
Furiosa Compiler tries to hide IO times between DRAM and SRAM by overlapping
them with NPU executions. In other words, NPU can execute operators while I/O operations are working.
If we split a large tensor into a number of smaller tensors,
the number of I/O operations can be hidden by NPU executions.</p>
<p>Once we decide to use this optimization, the next step is to determine the patch size.
Here, one good metric to determine the patch size is the ratio of the time spent on NPU executions.
The smaller the patch size, the more time is spent on NPU computation.
In contrast, the larger the patch size, the more time is spent on I/O operations.</p>
<p>Also, this optimization can be combined with using multiple NPU devices.
The multiple patches can run across multiple NPU devices in parallel.</p>
</section>
<section id="more-batch-more-npu-utilization">
<h3>More Batch, More NPU Utilization<a class="headerlink" href="#more-batch-more-npu-utilization" title="Permalink to this heading"></a></h3>
<p>For some models with small weights or few layers, the NPU utilization may be low.
In this case, we can increase the batch size to make the NPU utilization higher.
With this optimization, the inference may still have the same latency,
but its throughput can be increased significantly.</p>
<p>A batch size can be specified when compiling a model with <code class="docutils literal notranslate"><span class="pre">--batch-size</span></code> option as follows:</p>
<p><code class="docutils literal notranslate"><span class="pre">furiosa-compiler</span> <span class="pre">--batch-size</span> <span class="pre">32</span> <span class="pre">--target-npu</span> <span class="pre">warboy</span> <span class="pre">mnist.dfg</span> <span class="pre">-o</span> <span class="pre">mnist.enf</span></code></p>
<p>A batch size also can be specified when creating a session with <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> option.
You can learn more about the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> option from
<a class="reference external" href="https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#furiosa.runtime.furiosa.runtime.Runtime.create_runner">Runner API</a>.</p>
</section>
<section id="single-pe-vs-fusion-pe">
<h3>Single PE vs Fusion PE<a class="headerlink" href="#single-pe-vs-fusion-pe" title="Permalink to this heading"></a></h3>
<p>A single Warboy chip consists of two processing elements (PEs).
Each PE of Warboy has its own control unit, and the two PEs can work independently.
In this mode, each PE works with spatially-partitioned memory and processing units.
In contrast, two PEs can also be fused as a single PE.
In this fused mode, two PEs work as a single PE with an unified memory and processing units.</p>
<p>These two modes allow applications to have more flexibility to optimize the performance.
For example, if a model has large weights, we can use a 2PE-fused mode to load the weights
for a lower latency. If a model fits in a single PE, we can use two single PEs separately
to run the two model instances for higher throughput.</p>
<p>If a workload is latency-oriented, using a 2PE-fused mode is generally recommended.
If a workload is throughput-oriented, using two single PEs is generally recommended.
It still depends on models and workloads.
You need to find the optimal NPU configuration through experiments.</p>
<p>The followings are example commands to compile a model with a single PE or a fused-PE respectively.</p>
<ul class="simple">
<li><p>Single PE: <code class="docutils literal notranslate"><span class="pre">furiosa-compiler</span> <span class="pre">--target-npu</span> <span class="pre">warboy</span> <span class="pre">resnet50.dfg</span> <span class="pre">-o</span> <span class="pre">resnet50.enf</span></code></p></li>
<li><p>Fusion PE: <code class="docutils literal notranslate"><span class="pre">furiosa-compiler</span> <span class="pre">--target-npu</span> <span class="pre">warboy-2pe</span> <span class="pre">resnet50.dfg</span> <span class="pre">-o</span> <span class="pre">resnet50_2pe.enf</span></code></p></li>
</ul>
<p>This NPU configuration can also be specified
when creating a <a class="reference external" href="https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#furiosa.runtime.Runtime">Runtime</a>
with <code class="docutils literal notranslate"><span class="pre">device</span></code> option which are specified by
<a class="reference external" href="https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#device-specification">Device Configuration</a></p>
</section>
</section>
<section id="runtime-optimization">
<h2>Runtime Optimization<a class="headerlink" href="#runtime-optimization" title="Permalink to this heading"></a></h2>
<p>So far, we have discussed the model optimization techniques to reduce the inference latency.
After we apply the model optimization, we can futher optimize the performance in Runtime level.</p>
<p>As we mentioned above, an end-to-end inference consists of three operations:
NPU execution, CPU computation, and IO operation.
Three kinds of operations can run independently without blocking one another.
They can be overlapped if we run multiple inferences simultaneously.
Leveraging this characteristic is a key idea of the runtime optimization.</p>
<section id="more-inference-concurrency-the-number-of-workers">
<span id="concurrencyoptimization"></span><h3>More inference concurrency (the number of workers)<a class="headerlink" href="#more-inference-concurrency-the-number-of-workers" title="Permalink to this heading"></a></h3>
<p>When we create a session through <a class="reference external" href="https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#furiosa.runtime.furiosa.runtime.Runtime.create_runner">Runner API</a>,
we can specify the number of workers as an option.
A single worker is a unit that can run inferences independently sharing NPUs.
This concept is similar to a thread and CPUs.</p>
<p>If there is only one worker, multiple inference requests are processed sequentially through a single worker.
When one inference is completed, the next inference is processed by the owrker.
In this case, the NPU can be idle while the CPU is working, causing low NPU utilization.</p>
<figure class="align-center">
<img alt="Single Worker" class="with-shadow" src="../_images/worker_single.png" />
</figure>
<p></p>
<p>However, if there are multiple workers, the workers consume requests from the request queue
in Runtime. The multiple inferences can be processed simultaneously.
In this case, NPU executions are overlapped with CPU executions, possibly leading to higher NPU utilization.</p>
<figure class="align-center">
<img alt="Multiple Workers" class="with-shadow" src="../_images/worker_multiple.png" />
</figure>
<p></p>
<p>Each worker requires more memory resources to maintain context information for its execution.
If the number of workers is too large, the memory resources may be exhausted.
If the number of workers is too small, the NPU utilization may be low.
Finding the optimal number of workers is important to maximize the performance of the model.
Usually, we can find the optimal number of workers through experimentation.</p>
</section>
<section id="sync-api-vs-async-apis">
<h3>Sync API vs Async APIs<a class="headerlink" href="#sync-api-vs-async-apis" title="Permalink to this heading"></a></h3>
<p>There are two types of runtime APIs: Sync API and Async API.
Sync API is a blocking API that waits for the completion of the inference.
Async APIs are non-blocking APIs that don’t wait for the completion of the inference.
Async APIs allow to request multiple inferences simultaneously and wait for the results asynchronously.</p>
<p><code class="docutils literal notranslate"><span class="pre">furiosa.session.create()</span></code> a creates a syncronous session.
As the below example, <code class="docutils literal notranslate"><span class="pre">session.run()</span></code> is blocked until the result is returned.
It generally is enough for batch workloads with large batch sizes,
but it’s not sufficient for serving workloads that handle multiple current requests simultaneously.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">furiosa.runtime</span> <span class="kn">import</span> <span class="n">session</span>

<span class="k">with</span> <span class="n">session</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="c1"># Wait for completion</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>To overcome this limitation, Furiosa SDK provides two types of Async APIs: Queue API and Async/Await API.
They allow to request multiple inferences simultaneously and wait for the results asynchronously.
They are also useful to hide I/O and CPU computation by overlapping them with NPU executions.</p>
<section id="queue-api">
<h4>Queue API<a class="headerlink" href="#queue-api" title="Permalink to this heading"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">create_async()</span></code> creates a pair of a submitter and a queue.
With both, we can submit inference requests without waiting for completion and
wait for the inference results asynchronously.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="kn">from</span> <span class="nn">furiosa.runtime</span> <span class="kn">import</span> <span class="n">session</span>

<span class="n">submitter</span><span class="p">,</span> <span class="n">queue</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">create_async</span><span class="p">(</span><span class="s2">&quot;mnist.onnx&quot;</span><span class="p">,</span>
                                        <span class="n">worker_num</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                        <span class="c1"># Determine how many asynchronous requests you can submit</span>
                                        <span class="c1"># without blocking.</span>
                                        <span class="n">input_queue_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                        <span class="n">output_queue_size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">59999</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">submitter</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">idx</span><span class="p">)</span> <span class="c1"># non blocking call</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">context</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="c1"># 100 ms for timeout. If None, queue.recv() will be blocking.</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="k">if</span> <span class="n">queue</span><span class="p">:</span>
    <span class="n">queue</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="k">if</span> <span class="n">submitter</span><span class="p">:</span>
    <span class="n">submitter</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="using-async-await-syntax">
<h4>Using Async/Await syntax<a class="headerlink" href="#using-async-await-syntax" title="Permalink to this heading"></a></h4>
<p>In the the example below, <code class="docutils literal notranslate"><span class="pre">NPUModel</span></code> of furiosa-server provide an easier way to implement
a serving application using async/await API.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">furiosa.server.model</span> <span class="kn">import</span> <span class="n">NPUModel</span><span class="p">,</span> <span class="n">NPUModelConfig</span>

<span class="k">class</span> <span class="nc">SimpleApplication</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">NPUModel</span><span class="p">(</span>
            <span class="n">NPUModelConfig</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;MNIST&quot;</span><span class="p">,</span>
                <span class="n">model</span><span class="o">=</span><span class="s2">&quot;mnist.onnx&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">postprocess</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
        <span class="c1"># do preprocess</span>
        <span class="k">return</span> <span class="n">image</span>

    <span class="k">def</span> <span class="nf">postprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
        <span class="c1"># do postprocess</span>
        <span class="k">return</span> <span class="n">tensor</span>


<span class="n">APP</span> <span class="o">=</span> <span class="n">SimpleApplication</span><span class="p">()</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">startup</span><span class="p">():</span>
    <span class="k">await</span> <span class="n">APP</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">image</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">APP</span><span class="o">.</span><span class="n">process</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">asyncio</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">startup</span><span class="p">())</span>

    <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">asyncio</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">run</span><span class="p">(</span><span class="n">image</span><span class="p">))</span>
</pre></div>
</div>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="quantization.html" class="btn btn-neutral float-left" title="Model Quantization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="profiler.html" class="btn btn-neutral float-right" title="Performance Profiling" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023 FuriosaAI, Inc..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>