<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Model Server (Serving Framework) &mdash; Furiosa SDK Documentation furiosa-docs documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/tabs.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Tutorial and Code Examples" href="tutorials.html" />
    <link rel="prev" title="Kubernetes Support" href="kubernetes_support.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Furiosa SDK Documentation
          </a>
              <div class="version">
                0.8.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">FuriosaAI NPU</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../npu/intro.html">FuriosaAI NPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../npu/intro.html#furiosaai-warboy">FuriosaAI Warboy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../npu/supported_operators.html">List of Supported Operators for NPU Acceleration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FuriosaAI Software</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="intro.html">FuriosaAI SW Stack Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Driver, Firmware, and Runtime Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="python-sdk.html">Python SDK installation and user guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="c-sdk.html">C SDK installation and user guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command Line Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiler.html">Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Model Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">Performance profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="kubernetes_support.html">Kubernetes Support</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Model Server (Serving Framework)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#installation">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-a-model-server">Running a Model Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-a-model-server-with-a-configuration-file">Running a Model Server with a Configuration File</a></li>
<li class="toctree-l2"><a class="reference internal" href="#endpoints">Endpoints</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorial and Code Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../releases/0.8.0.html">Release Notes - 0.8.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases/0.7.0.html">Release Notes - 0.7.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases/0.6.0.html">Release Notes - 0.6.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases/0.5.0.html">Release Notes - 0.5.0</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Customer Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.atlassian.net/servicedesk/customer/portals/">FuriosaAI Customer Center</a></li>
<li class="toctree-l1"><a class="reference internal" href="../customer-support/bugs.html">Bug Report</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Previous Documents</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/docs/v0.7.0/en/">v0.7.0</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/docs/v0.6.0/en/">v0.6.0</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/docs/v0.5.0/en/">v0.5.0</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/docs/v0.2.0/en/">v0.2.0</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Furiosa SDK Documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a></li>
      <li class="breadcrumb-item active">Model Server (Serving Framework)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/software/serving.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="model-server-serving-framework">
<span id="modelserving"></span><h1>Model Server (Serving Framework)<a class="headerlink" href="#model-server-serving-framework" title="Permalink to this headline"></a></h1>
<p>To serve DNN models through GRPC and REST API, you can use <a class="reference external" href="https://github.com/furiosa-ai/furiosa-sdk/tree/main/python/furiosa-server">Furiosa Model Server</a>.
Model Server provides the endpoints compatible with <a class="reference external" href="https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md">KServe Predict Protocol Version 2</a>.</p>
<p>Its major features are:</p>
<blockquote>
<div><ul class="simple">
<li><p>REST/GRPC endpoints support</p></li>
<li><p>Multiple model serving using multiple NPU devices</p></li>
</ul>
</div></blockquote>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline"></a></h2>
<p>Its requirements are:</p>
<ul class="simple">
<li><p>Ubuntu 18.04 LTS (Debian buster) or higher</p></li>
<li><p><a class="reference internal" href="installation.html#requiredpackages"><span class="std std-ref">Driver, Firmware, and Runtime Installation</span></a></p></li>
<li><p>Python 3.7 or higher version</p></li>
</ul>
<p>If you need Python environment, please refer to <a class="reference internal" href="python-sdk.html#setuppython"><span class="std std-ref">Python execution environment setup</span></a> first.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">Installation using PIP</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">Installation from source code</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><p>Run the following command</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ pip install <span class="s1">&#39;furiosa-sdk[server]&#39;</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><p>Check out the source code and run the following command</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ git clone https://github.com/furiosa-ai/furiosa-sdk.git
$ <span class="nb">cd</span> furiosa-sdk/python/furiosa-server
$ pip install .
</pre></div>
</div>
</div></div>
</div>
<div class="section" id="running-a-model-server">
<h2>Running a Model Server<a class="headerlink" href="#running-a-model-server" title="Permalink to this headline"></a></h2>
<p>You can run model sever command by running <code class="docutils literal notranslate"><span class="pre">furiosa</span> <span class="pre">server</span></code> in your shell.</p>
<p>To run simply a model server with <code class="docutils literal notranslate"><span class="pre">tflite</span></code> or <code class="docutils literal notranslate"><span class="pre">onnx</span></code>, you need to specify
just the model path and its name as following:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ <span class="nb">cd</span> furiosa-sdk
$ furiosa server <span class="se">\</span>
--model-path examples/assets/quantized_models/MNISTnet_uint8_quant_without_softmax.tflite <span class="se">\</span>
--model-name mnist
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">--model-path</span></code> option allows to specify a path of a model file.
If you want to use a specific binding address and port, you can use additionally
<code class="docutils literal notranslate"><span class="pre">--host</span></code>, <code class="docutils literal notranslate"><span class="pre">--host-port</span></code>.</p>
<p>Please run <code class="docutils literal notranslate"><span class="pre">furiosa</span> <span class="pre">server</span> <span class="pre">--help</span></code> if you want to learn more
about the command with various options.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ furiosa server --help
Usage: furiosa server <span class="o">[</span>OPTIONS<span class="o">]</span>

    Start serving models from FuriosaAI model server

Options:
    --log-level <span class="o">[</span>ERROR<span class="p">|</span>INFO<span class="p">|</span>WARN<span class="p">|</span>DEBUG<span class="p">|</span>TRACE<span class="o">]</span>
                                    <span class="o">[</span>default: LogLevel.INFO<span class="o">]</span>
    --model-path TEXT               Path to Model file <span class="o">(</span>tflite, onnx are
                                    supported<span class="o">)</span>
    --model-name TEXT               Model name used <span class="k">in</span> URL path
    --model-version TEXT            Model version used <span class="k">in</span> URL path  <span class="o">[</span>default:
                                    default<span class="o">]</span>
    --host TEXT                     IP address to <span class="nb">bind</span>  <span class="o">[</span>default: <span class="m">0</span>.0.0.0<span class="o">]</span>
    --http-port INTEGER             HTTP port to listen to requests  <span class="o">[</span>default:
                                    <span class="m">8080</span><span class="o">]</span>
    --model-config FILENAME         Path to a config file about models with
                                    specific configurations
    --server-config FILENAME        Path to Model file <span class="o">(</span>tflite, onnx are
                                    supported<span class="o">)</span>
    --install-completion <span class="o">[</span>bash<span class="p">|</span>zsh<span class="p">|</span>fish<span class="p">|</span>powershell<span class="p">|</span>pwsh<span class="o">]</span>
                                    Install completion <span class="k">for</span> the specified shell.
    --show-completion <span class="o">[</span>bash<span class="p">|</span>zsh<span class="p">|</span>fish<span class="p">|</span>powershell<span class="p">|</span>pwsh<span class="o">]</span>
                                    Show completion <span class="k">for</span> the specified shell, to
                                    copy it or customize the installation.
    --help                          Show this message and exit.
</pre></div>
</div>
</div>
<div class="section" id="running-a-model-server-with-a-configuration-file">
<h2>Running a Model Server with a Configuration File<a class="headerlink" href="#running-a-model-server-with-a-configuration-file" title="Permalink to this headline"></a></h2>
<p>If you need more advanced configurations like compilation options and device options,
you can use a configuration file based on Yaml.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">model_config_list</span><span class="p">:</span><span class="w"></span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mnist</span><span class="w"></span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">path</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;samples/data/MNISTnet_uint8_quant.tflite&quot;</span><span class="w"></span>
<span class="w">    </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">    </span><span class="nt">npu_device</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">npu0pe0</span><span class="w"></span>
<span class="w">    </span><span class="nt">compiler_config</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">keep_unsignedness</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
<span class="w">        </span><span class="nt">split_unit</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span><span class="w"></span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ssd</span><span class="w"></span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">path</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;samples/data/tflite/SSD512_MOBILENET_V2_BDD_int_without_reshape.tflite&quot;</span><span class="w"></span>
<span class="w">    </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">    </span><span class="nt">npu_device</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">npu0pe1</span><span class="w"></span>
</pre></div>
</div>
<p>When you run a model sever with a configuration file,
you need to specify <code class="docutils literal notranslate"><span class="pre">--model-config</span></code> as following.
You can find the model files described in the above example from
<a class="reference external" href="https://github.com/furiosa-ai/furiosa-sdk/tree/main/python/furiosa-server/samples">furiosa-models/samples</a>.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ <span class="nb">cd</span> furiosa-sdk/python/furiosa-server
$ furiosa server --model-config samples/model_config_example.yaml

Saving the compilation log into /Users/hyunsik/.local/state/furiosa/logs/compile-20211126143917-2731kz.log
Using furiosa-compiler <span class="m">0</span>.5.0 <span class="o">(</span>rev: 407c0c51f built at <span class="m">2021</span>-11-26 <span class="m">12</span>:05:30<span class="o">)</span>
<span class="m">2021</span>-11-26T22:39:17.819518Z  INFO Npu <span class="o">(</span>npu0pe0<span class="o">)</span> is being initialized
<span class="m">2021</span>-11-26T22:39:17.823511Z  INFO NuxInner create with pes: <span class="o">[</span>PeId<span class="o">(</span><span class="m">0</span><span class="o">)]</span>
...
INFO:     Started server process <span class="o">[</span><span class="m">62087</span><span class="o">]</span>
INFO:uvicorn.error:Started server process <span class="o">[</span><span class="m">62087</span><span class="o">]</span>
INFO:     Waiting <span class="k">for</span> application startup.
INFO:uvicorn.error:Waiting <span class="k">for</span> application startup.
INFO:     Application startup complete.
INFO:uvicorn.error:Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8080 <span class="o">(</span>Press CTRL+C to quit<span class="o">)</span>
INFO:uvicorn.error:Uvicorn running on http://0.0.0.0:8080 <span class="o">(</span>Press CTRL+C to quit<span class="o">)</span>
</pre></div>
</div>
<p>Once a model server starts up, you can call the inference request through HTTP protocol.
If the model name is <code class="docutils literal notranslate"><span class="pre">mnist</span></code> and its version <code class="docutils literal notranslate"><span class="pre">1</span></code>, the endpoint of the model will be
<code class="docutils literal notranslate"><span class="pre">http://&lt;host&gt;:&lt;port&gt;/v2/models/mnist/version/1/infer</span></code>, accepting <code class="docutils literal notranslate"><span class="pre">POST</span></code> http request.
The following is an example using <code class="docutils literal notranslate"><span class="pre">curl</span></code> to send the inference request and return the response.</p>
<p>The following is a Python example, doing same as <code class="docutils literal notranslate"><span class="pre">curl</span></code> does in the above example.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">mnist</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">mnist_images</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train_images</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">60000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;http://localhost:8080/v2/models/mnist/versions/1/infer&#39;</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">mnist_images</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">request</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[{</span>
        <span class="s2">&quot;name&quot;</span><span class="p">:</span>
        <span class="s2">&quot;mnist&quot;</span><span class="p">,</span>
        <span class="s2">&quot;datatype&quot;</span><span class="p">:</span> <span class="s2">&quot;UINT8&quot;</span><span class="p">,</span>
        <span class="s2">&quot;shape&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span>
        <span class="s2">&quot;data&quot;</span><span class="p">:</span> <span class="n">data</span>
    <span class="p">}]</span>
<span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">request</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="section" id="endpoints">
<h2>Endpoints<a class="headerlink" href="#endpoints" title="Permalink to this headline"></a></h2>
<p>The following table shows REST API endpoints and its descriptions.
The model server is following KServe Predict Protocol Version 2.
So, you can find more details from <a class="reference external" href="https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#httprest">KServe Predict Protocol Version 2 - HTTP/REST</a>.</p>
<table class="colwidths-given docutils align-default" id="id1">
<caption><span class="caption-text">Endpoints of KServe Predict Protocol Version 2</span><a class="headerlink" href="#id1" title="Permalink to this table"></a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Method and Endpoint</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>GET /v2/health/live</p></td>
<td><p>Returns HTTP Ok (200) if the inference server is able to receive and respond to metadata and inference requests.
This API can be directly used for the Kubernetes livenessProbe.</p></td>
</tr>
<tr class="row-odd"><td><p>GET /v2/health/ready</p></td>
<td><p>Returns HTTP Ok (200) if all the models are ready for inferencing.
This API can be directly used for the Kubernetes readinessProbe.</p></td>
</tr>
<tr class="row-even"><td><p>GET /v2/models/${MODEL_NAME}/versions/${MODEL_VERSION}</p></td>
<td><p>Returns a model metadata</p></td>
</tr>
<tr class="row-odd"><td><p>GET /v2/models/${MODEL_NAME}/versions/${MODEL_VERSION}/ready</p></td>
<td><p>Returns HTTP Ok (200) if a specific model is ready for inferencing.</p></td>
</tr>
<tr class="row-even"><td><p>POST /v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/infer</p></td>
<td><p>Inference request</p></td>
</tr>
</tbody>
</table>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="kubernetes_support.html" class="btn btn-neutral float-left" title="Kubernetes Support" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="tutorials.html" class="btn btn-neutral float-right" title="Tutorial and Code Examples" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022 FuriosaAI, Inc..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>